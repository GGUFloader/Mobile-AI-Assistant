# LocalChatbot - On-Device AI Assistant for Arm

> ğŸ† **Arm AI Developer Challenge 2025 Submission**

[![Download APK](https://img.shields.io/badge/Download-APK%20v0.0.1-green?style=for-the-badge&logo=android)](https://github.com/GGUFloader/Mobile-AI-Assistant/releases/download/0.0.1/mobile.ai.assistant.beta.v.0.0.1.apk)

> âš ï¸ **Before Installing APK:** Disable Google Play Protect temporarily:
> Settings â†’ Google â†’ Security â†’ Google Play Protect â†’ Turn off "Scan apps with Play Protect"
> (You can re-enable it after installation)

**Your AI, Your Model, Your Device** â€” A fully offline, privacy-first AI chatbot that lets you run ANY large language model locally on Arm-powered Android devices. Choose the perfect model for your hardware and task.

### ğŸ“º Demo Video

[![Watch Demo Video](https://img.youtube.com/vi/myDmYMXoG5A/maxresdefault.jpg)](https://youtube.com/shorts/myDmYMXoG5A)

> ğŸ‘† Click the image above to watch the demo on YouTube

![LocalChatbot Logo](logo.png)

## ğŸ¯ Project Overview

LocalChatbot brings the power of generative AI directly to your pocket with **unprecedented flexibility**. Unlike other mobile AI apps that lock you into a single model, LocalChatbot empowers users to:

- ğŸ“± **Choose models that match your device** â€” Run lightweight 0.5B models on budget phones or powerful 7B+ models on flagships
- ğŸ”„ **Switch models for different tasks** â€” Use a coding-optimized model for programming, a creative model for writing, or a general assistant for everyday questions  
- ğŸ’¾ **Optimize for your resources** â€” Select quantization levels (Q4, Q5, Q8) based on available RAM and desired quality
- ğŸŒ **Stay completely offline** â€” All inference happens on-device with zero data transmission

The app leverages Arm's NEON SIMD instructions for optimized inference, making it possible to run quantized LLMs on mobile devices with impressive performance.

### Why This Project Should Win

| Criteria | How LocalChatbot Delivers |
|----------|---------------------------|
| **Technological Implementation** | Native C++/JNI integration with llama.cpp, ARM NEON SIMD optimizations, multi-engine architecture supporting GGUF and ExecuTorch formats |
| **User Experience** | **Flexible model selection** lets users choose models based on their device resources; Material 3 design, streaming responses, floating system-wide assistant |
| **Potential Impact** | Democratizes on-device AI â€” users with ANY Arm phone can run AI by selecting appropriate models for their hardware |
| **WOW Factor** | One app, unlimited models â€” load a tiny 0.5B model on a budget phone or a powerful 7B model on a flagship, all offline! |

### Key Innovations

1. **Flexible Model Selection** â­: Users can load ANY GGUF model based on their device capabilities â€” choose lightweight models for older phones or powerful models for flagship devices. Switch models anytime for different tasks (coding, writing, Q&A)
2. **True Privacy**: All AI processing happens locally â€” your conversations never leave your device
3. **Arm-Native Optimization**: Built from the ground up to leverage Arm NEON SIMD for maximum performance
4. **System-Wide AI Access**: Floating assistant and text selection integration bring AI to any app
5. **Multi-Engine Support**: Supports both GGUF (llama.cpp) and ExecuTorch (.pte) model formats
6. **Production-Ready UX**: Polished Material 3 UI with streaming responses, resource monitoring, and intuitive controls

## âœ¨ Key Features

### ğŸ”„ Flexible Model Selection (Key Feature!)
- **Load any GGUF model** â€” no hardcoded models, full user control
- **Resource-aware choices**: Pick models that fit YOUR device's RAM and CPU
- **Task-specific models**: Use a coding model for programming, a chat model for conversations
- **Hot-swap models**: Change models without reinstalling the app
- **Wide compatibility**: From 0.5B models on budget phones to 7B+ on flagships

### ğŸ¤– Local LLM Inference
- Run quantized GGUF models (Q4, Q5, Q8) directly on device
- ExecuTorch support for Meta's optimized mobile models
- Streaming token generation for responsive UX
- Conversation context management

### ğŸˆ Floating AI Assistant
- System-wide floating button accessible from any app
- Draggable chat window overlay
- Drag-to-close gesture for easy dismissal
- Persistent across app switches

### ğŸ“ Text Selection Integration
- Select text anywhere â†’ "Ask AI" appears in context menu
- Instant AI analysis of selected content
- Copy response to clipboard

### ğŸ“Š Real-Time Resource Monitoring
- Live CPU usage tracking
- Memory consumption display
- Native heap monitoring for model memory
- Toggle stats on/off for performance

### âš¡ Arm Architecture Optimizations
- ARM64-v8a native build with NEON SIMD enabled
- Compiler flags: `-O3 -ffast-math -march=armv8-a+simd`
- Greedy sampling (temperature=0) for fastest inference
- Minimal context window for mobile efficiency


## ğŸ› ï¸ Technical Implementation

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      LocalChatbot App                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  UI Layer (Jetpack Compose + Material 3)                    â”‚
â”‚  â”œâ”€â”€ ChatScreen - Main chat interface                       â”‚
â”‚  â”œâ”€â”€ SettingsScreen - Model & inference settings            â”‚
â”‚  â”œâ”€â”€ FloatingAssistantService - System overlay              â”‚
â”‚  â””â”€â”€ ProcessTextActivity - Text selection handler           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Business Logic                                              â”‚
â”‚  â”œâ”€â”€ ChatViewModel - UI state management                    â”‚
â”‚  â”œâ”€â”€ ModelRunner - Inference orchestration                  â”‚
â”‚  â””â”€â”€ ResourceMonitor - Performance tracking                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Inference Layer                                             â”‚
â”‚  â”œâ”€â”€ EngineManager - Multi-engine abstraction               â”‚
â”‚  â”œâ”€â”€ GGUFEngine - llama.cpp integration                     â”‚
â”‚  â””â”€â”€ ExecuTorchEngine - Meta ExecuTorch support             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Native Layer (C++ / JNI)                                    â”‚
â”‚  â”œâ”€â”€ llama-android.cpp - JNI bindings                       â”‚
â”‚  â””â”€â”€ llama.cpp - Optimized inference engine                 â”‚
â”‚      â””â”€â”€ GGML with ARM NEON SIMD                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Arm-Specific Optimizations

**Why Arm NEON Matters:**
ARM NEON is a SIMD (Single Instruction, Multiple Data) architecture extension that allows parallel processing of multiple data elements. For LLM inference, this means:
- Matrix multiplications run 4-8x faster
- Quantized model operations are hardware-accelerated
- Memory bandwidth is used more efficiently

**CMake Configuration:**
```cmake
# ARM NEON SIMD optimizations (critical for mobile performance)
set(GGML_NEON ON CACHE BOOL "Enable ARM NEON" FORCE)

# Disable unnecessary features for mobile
set(LLAMA_CURL OFF)
set(GGML_OPENMP OFF)  # Single-threaded for battery efficiency

# Performance flags
set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -O3 -ffast-math -fno-finite-math-only")
```

**Gradle NDK Configuration:**
```kotlin
ndk {
    abiFilters += listOf("arm64-v8a")  // ARM64 only - no x86 bloat
}
externalNativeBuild {
    cmake {
        arguments += "-DGGML_NEON=ON"
        cppFlags += listOf("-O3", "-ffast-math", "-march=armv8-a+simd")
    }
}
```

**Inference Optimizations:**
- Greedy sampling (temperature=0, top_k=1) for fastest token selection
- 2048 token context window optimized for mobile memory
- Single-turn conversation history to minimize prompt size
- Streaming generation for perceived responsiveness

## ğŸ“± Setup Instructions

### Prerequisites
- Android Studio Hedgehog (2023.1.1) or newer
- Android NDK 25.0 or newer (installed via SDK Manager)
- CMake 3.22.1+ (installed via SDK Manager)
- An Arm64 Android device (arm64-v8a) with Android 8.0+
- ~2GB free storage for models

### Build Steps

1. **Clone the repository:**
   ```bash
   git clone https://github.com/GGUFloader/Mobile-AI-Assistant.git
   cd Mobile-AI-Assistant
   ```

2. **Open in Android Studio:**
   - Open the `Mobile-AI-Assistant` folder in Android Studio
   - Wait for Gradle sync to complete (llama.cpp is automatically downloaded by Gradle)
   - If prompted, install any missing SDK components
   - If prompted, install any missing SDK components

3. **Build the project:**
   ```bash
   # On Windows
   gradlew.bat assembleRelease
   
   # On macOS/Linux
   ./gradlew assembleRelease
   ```
   Or use Android Studio: Build â†’ Build Bundle(s) / APK(s) â†’ Build APK(s)

4. **Install on device:**
   ```bash
   adb install app/build/outputs/apk/release/app-release.apk
   ```

### Running the App

1. **Download a GGUF model** (choose based on your device RAM):
   
   | Model | Size | RAM Required | Download |
   |-------|------|--------------|----------|
   | **Dhi Qwen2.5 0.5B Q4_K_M** â­ | ~400MB | 512MB | [Download](https://huggingface.co/mathanmithun/DhiGrowth_FAQ_RAG/resolve/main/models/Dhi_Qwen2P5_0_5B_Q4_K_M.gguf) |
   | TinyLlama 1.1B Q4_K_M | 637MB | 1GB | [HuggingFace](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF) |
   | Phi-2 Q4_K_M | 1.6GB | 2GB | [HuggingFace](https://huggingface.co/TheBloke/phi-2-GGUF) |
   | Gemma 2B Q4_K_M | 1.4GB | 2GB | [HuggingFace](https://huggingface.co/google/gemma-2b-it-GGUF) |

   > ğŸ’¡ **Recommended for Testing:** The Dhi Qwen2.5 0.5B model is lightweight and perfect for quick testing on most devices.

2. **Transfer model to device:**
   ```bash
   # Recommended model for testing
   adb push Dhi_Qwen2P5_0_5B_Q4_K_M.gguf /sdcard/Download/
   
   # Or any other model you downloaded
   adb push TinyLlama-1.1B-Chat-v1.0-Q4_K_M.gguf /sdcard/Download/
   ```

3. **Load model in app:**
   - Open LocalChatbot
   - Tap the folder icon to select your model file
   - Wait for model to load (progress shown)

4. **Start chatting!**
   - Type your message and tap send
   - Enable floating button for system-wide access


## ğŸ® Usage Guide

### Main Chat Interface
- Type messages in the input field
- Tap send or press enter to generate response
- Watch streaming tokens appear in real-time
- Tap stop button to cancel generation mid-stream
- Toggle stats icon to see CPU/memory usage

### Floating Assistant (System-Wide AI)
1. Enable "Floating Button" toggle in main screen
2. Grant overlay permission when prompted
3. Tap floating bubble from any app to open chat
4. Drag bubble to reposition
5. Drag bubble to top of screen to dismiss

### Text Selection AI
1. Select text in any app (browser, email, notes, etc.)
2. Tap "Ask AI" from the context menu
3. View AI response in popup dialog
4. Copy response to clipboard with one tap

### Settings
- Adjust inference parameters (temperature, top_k, top_p)
- Configure context window size
- Switch between inference engines
- Manage loaded models

## ğŸ“Š Performance Benchmarks

Tested on various Arm devices:

| Device | SoC | Model | Tokens/sec | Memory |
|--------|-----|-------|------------|--------|
| Pixel 7 | Tensor G2 (Cortex-X1) | TinyLlama 1.1B Q4 | 8-12 t/s | ~800MB |
| Pixel 7 | Tensor G2 | Phi-2 Q4 | 4-6 t/s | ~1.8GB |
| Samsung S23 | Snapdragon 8 Gen 2 | TinyLlama 1.1B Q4 | 10-15 t/s | ~800MB |
| OnePlus 12 | Snapdragon 8 Gen 3 | Gemma 2B Q4 | 8-10 t/s | ~1.6GB |

*Performance varies based on device thermal state and background processes*

## ğŸ”§ Project Structure

```
LocalChatbot/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ src/main/
â”‚   â”‚   â”œâ”€â”€ java/com/example/localchatbot/
â”‚   â”‚   â”‚   â”œâ”€â”€ MainActivity.kt              # App entry point
â”‚   â”‚   â”‚   â”œâ”€â”€ ModelRunner.kt               # Inference orchestration
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatApplication.kt           # App-wide state
â”‚   â”‚   â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ChatMessage.kt           # Message data class
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ChatHistoryRepository.kt # Persistence
â”‚   â”‚   â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ InferenceEngine.kt       # Engine interface
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ EngineManager.kt         # Multi-engine support
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ GGUFEngine.kt            # llama.cpp wrapper
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ExecuTorchEngine.kt      # ExecuTorch wrapper
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ LlamaCpp.kt              # JNI interface
â”‚   â”‚   â”‚   â”œâ”€â”€ overlay/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ FloatingAssistantService.kt  # Floating bubble
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ AssistantChatActivity.kt     # Overlay chat UI
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ProcessTextActivity.kt       # Text selection
â”‚   â”‚   â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ChatScreen.kt            # Main chat UI
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ChatViewModel.kt         # UI state management
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ SettingsScreen.kt        # Settings UI
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ theme/Theme.kt           # Material 3 theming
â”‚   â”‚   â”‚   â”œâ”€â”€ settings/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ InferenceSettings.kt     # Inference params
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ EngineSettings.kt        # Engine config
â”‚   â”‚   â”‚   â””â”€â”€ util/
â”‚   â”‚   â”‚       â””â”€â”€ ResourceMonitor.kt       # CPU/memory tracking
â”‚   â”‚   â”œâ”€â”€ cpp/
â”‚   â”‚   â”‚   â”œâ”€â”€ CMakeLists.txt               # Native build config
â”‚   â”‚   â”‚   â”œâ”€â”€ llama-android.cpp            # JNI bindings
â”‚   â”‚   â”‚   â””â”€â”€ llama.cpp/                   # Inference engine (submodule)
â”‚   â”‚   â”œâ”€â”€ res/                             # Android resources
â”‚   â”‚   â””â”€â”€ AndroidManifest.xml              # App manifest
â”‚   â””â”€â”€ build.gradle.kts                     # App build config
â”œâ”€â”€ build.gradle.kts                         # Root build config
â”œâ”€â”€ settings.gradle.kts                      # Gradle settings
â”œâ”€â”€ LICENSE                                  # MIT License
â””â”€â”€ README.md                                # This file
```

## ğŸ”’ Privacy & Security

LocalChatbot is designed with privacy as a core principle:

- **100% Offline**: No internet permission required, no data transmission
- **Local Processing**: All AI inference happens on-device
- **No Analytics**: Zero tracking, telemetry, or data collection
- **Open Source**: Full transparency â€” audit the code yourself
- **Your Data Stays Yours**: Conversations are stored locally and never leave your device

## ğŸš€ Future Roadmap

- [ ] Voice input/output support (on-device speech recognition)
- [ ] Multiple conversation threads with history
- [ ] In-app model download manager
- [ ] Prompt templates library
- [ ] Home screen widget for quick access
- [ ] Wear OS companion app
- [ ] RAG support for document Q&A

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues and pull requests.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) file for details.

This project is open source and free to use, modify, and distribute.

## ğŸ™ Acknowledgments

- [llama.cpp](https://github.com/ggerganov/llama.cpp) - The incredible inference engine that makes this possible
- [ExecuTorch](https://github.com/pytorch/executorch) - Meta's mobile inference framework
- [Arm](https://www.arm.com/) - For the amazing mobile architecture and NEON SIMD
- [Jetpack Compose](https://developer.android.com/jetpack/compose) - Modern Android UI toolkit

---

<p align="center">
  Built with â¤ï¸ for the <strong>Arm AI Developer Challenge 2025</strong>
</p>
